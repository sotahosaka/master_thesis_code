{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20aa3ca1-cba5-4ed0-a9f9-5629484a43e1",
   "metadata": {},
   "source": [
    "# Validation of labelling Methods\n",
    "## Comparing two proposed methods, mahalanobis base method and KRR+mahalanobis base method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e913df8-d614-4aba-b6b7-fb58c1ebc607",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff12a7e7-5577-4587-bd11-e643b0925db0",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320c686d-b6b9-4630-b9df-af09fdceccdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten, Input, Lambda\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import glob\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import time\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from keras.models import Model\n",
    "from gradcamutils import GradCam, GradCamPlusPlus, ScoreCam, GuidedBackPropagation, superimpose, read_and_preprocess_img, build_guided_model\n",
    "\n",
    "from affine_a_method_det import set_deterministic, fit_affine_A_method_deterministic, inverse_transform_ridge, mahalanobis_sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d00b7d5-eb9c-45b2-8c59-8850043e7309",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fuction for making label\n",
    "def create_image_labels(n_light, n_water, n_blackline, n_discoloration, n_dotsonline, n_adhesion, n_scratch):\n",
    "    \"\"\"\n",
    "    function for making label corresponding to each abnormal mode\n",
    "    \n",
    "    Args:\n",
    "        n_light (int): the number of light images\n",
    "        n_water (int): the number of water images\n",
    "        n_blackline (int): the number of black line images\n",
    "        n_discoloration (int): the number of discoloration images\n",
    "        n_dotsonline (int): the number of dots on line images\n",
    "        n_adhesion (int): the number of adhesion images\n",
    "        n_scrtch (int): the number of surface scratches images\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: 1collumn=image、2collumn=label\n",
    "    \"\"\"\n",
    "    # Name list for each abnormal mode\n",
    "    light_data = [f\"Light-{i}\" for i in range(1, n_light + 1)]\n",
    "    water_data = [f\"Water-{i}\" for i in range(1, n_water + 1)]\n",
    "    blackline_data = [f\"BlackLine-{i}\" for i in range(1, n_blackline + 1)]\n",
    "    discoloration_data = [f\"Discoloration-{i}\" for i in range(1, n_discoloration + 1)]\n",
    "    dotsonline_data = [f\"DotsOnLine-{i}\" for i in range(1, n_dotsonline + 1)]\n",
    "    copper_data = [f\"Adhesion-{i}\" for i in range(1, n_adhesion + 1)]\n",
    "    spark_data = [f\"SurfaceScratch-{i}\" for i in range(1, n_scratch + 1)]\n",
    "\n",
    "    # Label list\n",
    "    light_labels = [0] * n_light\n",
    "    water_labels = [1] * n_water\n",
    "    blackline_labels = [2] * n_blackline\n",
    "    discoloration_labels = [3] * n_discoloration\n",
    "    dotsonline_labels = [4] * n_dotsonline\n",
    "    adhesion_labels = [5] * n_adhesion\n",
    "    scratch_labels = [6] * n_scratch\n",
    "\n",
    "    # make dataframe by combining data\n",
    "    data = list(zip(light_data + water_data + blackline_data + discoloration_data + dotsonline_data + copper_data + spark_data, \n",
    "                    light_labels + water_labels + blackline_labels + discoloration_labels + dotsonline_labels + copper_labels + spark_labels))\n",
    "    df = pd.DataFrame(data, columns=[\"image\", \"label\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_7mode_images(src_dirs):\n",
    "    \"\"\"\n",
    "    src_dirs: list containing full pass of seven folders\n",
    "              ex: [\n",
    "                   r\"C:/path/mode0/*jpg\",\n",
    "                   r\"C:/path/mode1/*jpg\",\n",
    "                   ...\n",
    "                  ]\n",
    "\n",
    "    return:\n",
    "      all_images      :list of all images(cv2) \n",
    "      images_by_class : list of each abnormal mode images [list0, list1, ..., list6]\n",
    "      labels          : list of labels corresponding to each image\n",
    "      nums            : list of the number of each class images [n0,n1,...,n6]\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(src_dirs) == 7, \"Designate 7 folder pass\"\n",
    "\n",
    "    images_by_class = []\n",
    "    nums = []\n",
    "\n",
    "    # process\n",
    "    for i, path in enumerate(src_dirs):\n",
    "        filepaths = glob.glob(path)\n",
    "        print(f\"Class {i}: {len(filepaths)} files\")\n",
    "\n",
    "        imgs = []\n",
    "        for fp in filepaths:\n",
    "            img = cv2.imread(fp)\n",
    "            if img is not None:\n",
    "                imgs.append(img)\n",
    "\n",
    "        images_by_class.append(imgs)\n",
    "        nums.append(len(imgs))\n",
    "\n",
    "    # combine\n",
    "    all_images = []\n",
    "    for cls_imgs in images_by_class:\n",
    "        all_images.extend(cls_imgs)\n",
    "\n",
    "    # make label\n",
    "    labels = create_image_labels(nums) \n",
    "\n",
    "    return all_images, images_by_class, labels, nums\n",
    "\n",
    "# =========================\n",
    "# split the data to 7 clusters and make X_clusters / Y_clusters \n",
    "# =========================\n",
    "def prepare_clusters(source_features, target_feature, n_src=240, n_tgt=240, K=7):\n",
    "\n",
    "    X_all = np.asarray(source_features, dtype=np.float64)\n",
    "    Y_all = np.asarray(target_feature,   dtype=np.float64)\n",
    "    if X_all.shape[0] != n_src*K or Y_all.shape[0] != n_tgt*K:\n",
    "        raise ValueError(f\"the number of sample is different: X_all={X_all.shape}, Y_all={Y_all.shape}, Ideal=({n_src*K},{n_tgt*K})\")\n",
    "\n",
    "    X_clusters, Y_clusters = [], []\n",
    "    for k in range(K):\n",
    "        X_clusters.append(X_all[k*n_src:(k+1)*n_src])\n",
    "        Y_clusters.append(Y_all[k*n_tgt:(k+1)*n_tgt])\n",
    "    return X_clusters, Y_clusters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4693a3bc-3c3f-436a-8828-78653f63f23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get pass\n",
    "\n",
    "#wire raw images\n",
    "#TrainingData\n",
    "source_dirs = [\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "]\n",
    "#TestData\n",
    "source_test_dirs = [\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "]\n",
    "\n",
    "#bright images\n",
    "#TrainingData\n",
    "target1_dirs = [\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "]\n",
    "#TestData\n",
    "test_target1_dirs = [\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "]\n",
    "\n",
    "#camera dust images\n",
    "#TrainingData\n",
    "target2_dirs = [\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "]\n",
    "#TestData\n",
    "test_target2_dirs = [\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9312c7a1-b3ea-4828-a1d5-46ecaaa1d65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data and label\n",
    "source_all_imgs, source_imgs_by_class, source_label, source_num = load_7mode_images(source_dirs)\n",
    "source_test_all_imgs, source_test_imgs_by_class, source_test_label, source_test_num = load_7mode_images(source_test_dirs)\n",
    "target1_all_imgs, target1_imgs_by_class, target1_label, target1_num = load_7mode_images(target1_dirs)\n",
    "test_target1_all_imgs, test_target1_imgs_by_class, test_target1_label, test_target1_num = load_7mode_images(test_target1_dirs)\n",
    "target2_all_imgs, target2_imgs_by_class, target2_label, target2_num = load_7mode_images(target2_dirs)\n",
    "test_target2_all_imgs, test_target2_imgs_by_class, test_target2_label, test_target2_num = load_7mode_images(test_target2_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89f45d0-a791-4d49-9b42-3a01863272d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalization of images\n",
    "sourcefile_list = [file.astype(float)/255 for file in source_all_imgs]\n",
    "sourcefile_list = [cv2.resize(file, (360, 270)) for file in sourcefile_list]\n",
    "sourcefile_test_list = [file.astype(float)/255 for file in source_test_all_imgs]\n",
    "sourcefile_test_list = [cv2.resize(file, (360, 270)) for file in sourcefile_test_list]\n",
    "targetfile1_list = [file.astype(float)/255 for file in target1_all_imgs]\n",
    "targetfile1_list = [cv2.resize(file, (360, 270)) for file in targetfile1_list]\n",
    "test_targetfile1_list = [file.astype(float)/255 for file in test_target1_all_imgs]\n",
    "test_targetfile1_list = [cv2.resize(file, (360, 270)) for file in test_targetfile1_list]\n",
    "targetfile2_list = [file.astype(float)/255 for file in target2_all_imgs]\n",
    "targetfile2_list = [cv2.resize(file, (360, 270)) for file in targetfile2_list]\n",
    "test_targetfile2_list = [file.astype(float)/255 for file in test_target2_all_imgs]\n",
    "test_targetfile2_list = [cv2.resize(file, (360, 270)) for file in test_targetfile2_list]\n",
    "\n",
    "#numpy list\n",
    "original_source_label = source_label[\"label\"]\n",
    "original_source_label = np.array(original_source_label)\n",
    "original_source_test_label = source_test_label[\"label\"]\n",
    "original_source_test_label = np.array(original_source_test_label)\n",
    "original_target1_label = target1_label[\"label\"]\n",
    "original_target1_label = np.array(original_target1_label)\n",
    "original_test_target1_label = test_target1_label[\"label\"]\n",
    "original_test_target1_label = np.array(original_test_target1_label)\n",
    "original_target2_label = target2_label[\"label\"]\n",
    "original_target2_label = np.array(original_target2_label)\n",
    "original_test_target2_label = test_target2_label[\"label\"]\n",
    "original_test_target2_label = np.array(original_test_target2_label)\n",
    "\n",
    "\n",
    "#dummy parameter \n",
    "source_label = to_categorical(source_label[\"label\"])\n",
    "source_test_label = to_categorical(source_test_label[\"label\"])\n",
    "target1_label = to_categorical(target1_label[\"label\"])\n",
    "test_target1_label = to_categorical(test_target1_label[\"label\"])\n",
    "target2_label = to_categorical(target2_label[\"label\"])\n",
    "test_target2_label = to_categorical(test_target2_label[\"label\"])\n",
    "\n",
    "#change the data to numpy list\n",
    "#save original data\n",
    "raw_sourcefile_list = sourcefile_list\n",
    "raw_sourcefile_test_list = sourcefile_test_list\n",
    "raw_targetfile1_list = targetfile1_list\n",
    "raw_test_targetfile1_list = test_targetfile1_list\n",
    "raw_targetfile2_list = targetfile2_list\n",
    "raw_test_targetfile2_list = test_targetfile2_list\n",
    "\n",
    "#numpy list\n",
    "sourcefile_list = np.array(sourcefile_list)\n",
    "sourcefile_test_list = np.array(sourcefile_test_list)\n",
    "targetfile1_list = np.array(targetfile1_list)\n",
    "test_targetfile1_list = np.array(test_targetfile1_list)\n",
    "targetfile2_list = np.array(targetfile2_list)\n",
    "test_targetfile2_list = np.array(test_targetfile2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de62833e-d2c0-45bb-928b-f252b3988cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read base model\n",
    "base_model = load_model('BaseModel.h5')\n",
    "# separate feature extractor and output layer\n",
    "feature_output = base_model.layers[-5].output\n",
    "\n",
    "# GlobalAveragePooling\n",
    "pooled_output = layers.GlobalAveragePooling2D()(feature_output)\n",
    "\n",
    "# make feature extractor from base model\n",
    "feature_extractor = Model(inputs=base_model.input, outputs=pooled_output)\n",
    "feature_extractor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f07ee28-f26a-49ea-a629-8f4d85ba7c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract features(feature vector) from each image\n",
    "def extract_features(model, images):\n",
    "    return model.predict(images, batch_size=32)\n",
    "\n",
    "source_features = extract_features(feature_extractor, sourcefile_list)\n",
    "target1_features = extract_features(feature_extractor, targetfile1_list)\n",
    "target2_features = extract_features(feature_extractor, targetfile2_list)\n",
    "\n",
    "source_test_features = extract_features(feature_extractor, sourcefile_test_list)\n",
    "test_target1_features = extract_features(feature_extractor, test_targetfile1_list)\n",
    "test_target2_features = extract_features(feature_extractor, test_targetfile2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a2ad92-dc33-413f-b24f-cecf43a4fd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce the dimension from 1024 to 100\n",
    "n_comp = 100\n",
    "pca = PCA(n_components=n_comp, svd_solver='full', random_state=42)\n",
    "features_pca = pca.fit_transform(source_features)\n",
    "# mapping data to the pca space made above\n",
    "target1_features_pca = pca.transform(target1_features)\n",
    "target2_features_pca = pca.transform(target2_features)\n",
    "test_features_pca = pca.transform(source_test_features)\n",
    "test_target1_features_pca = pca.transform(test_target1_features)\n",
    "test_target2_features_pca = pca.transform(test_target2_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5487e2da-07b3-4654-86a1-ffd1fcb6e9a6",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60d6b73-d376-4b30-961b-9542719728aa",
   "metadata": {},
   "source": [
    "## Mahalanobis base labelling method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3301be9a-5dd6-4988-b624-41ae29ff564c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#semi-auto labelling to bright images\n",
    "# ===== calculate the center of clusters =====\n",
    "cluster_stats = {}\n",
    "for label in np.unique(original_target1_label):\n",
    "    cluster_data = target1_features_pca[original_target1_label == label]\n",
    "    center = np.mean(cluster_data, axis=0)\n",
    "    cov_matrix = np.cov(cluster_data, rowvar=False)\n",
    "    cov_inv = np.linalg.pinv(cov_matrix) \n",
    "    cluster_stats[label] = {\n",
    "        \"center\": center,\n",
    "        \"cov_inv\": cov_inv\n",
    "    }\n",
    "\n",
    "# ===== calculate distance between target1_features_pca and test_target1_features_pca =====\n",
    "raw_drift_scores = []\n",
    "target1_drift_scores = []\n",
    "target2_drift_scores = []\n",
    "results = []\n",
    "full_results = []\n",
    "\n",
    "print(\"Semi-auto labelling to bright images\")\n",
    "for i, vec in enumerate(test_target1_features_pca):  \n",
    "    distances = {}\n",
    "    for label, stats in cluster_stats.items():\n",
    "        center = stats[\"center\"]\n",
    "        cov_inv = stats[\"cov_inv\"]\n",
    "        dist = distance.mahalanobis(vec, center, cov_inv)\n",
    "        distances[label] = dist\n",
    "\n",
    "    min_dist_label = min(distances, key=distances.get)\n",
    "    min_dist = distances[min_dist_label]\n",
    "\n",
    "    results.append({\n",
    "        \"index\": i,\n",
    "        \"nearest_cluster\": min_dist_label,\n",
    "        \"mahalanobis_distance\": round(min_dist, 4)\n",
    "    })\n",
    "    full_results.append({\n",
    "        \"index\": i,\n",
    "        \"distance_from_0\": distances[0],\n",
    "        \"distance_from_1\": distances[1],\n",
    "        \"distance_from_2\": distances[2],\n",
    "        \"distance_from_3\": distances[3],\n",
    "        \"distance_from_4\": distances[4],\n",
    "        \"distance_from_5\": distances[5],\n",
    "        \"distance_from_6\": distances[6]\n",
    "    })\n",
    "\n",
    "df_labels = pd.DataFrame(results)[[\"nearest_cluster\"]].rename(columns={\"nearest_cluster\": \"pred_label\"})\n",
    "\n",
    "save_path = r\"C:\\Users\\pass\\example.csv\"\n",
    "df_labels.to_csv(save_path, index=False, encoding=\"utf-8-sig\")  \n",
    "print(\"CSV saved:\", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e582d87-0848-4323-bb0d-946abb3976e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#semi-auto labelling to camera dust images\n",
    "# ===== calculate the center of clusters =====\n",
    "cluster_stats = {}\n",
    "for label in np.unique(original_target2_label):\n",
    "    cluster_data = target2_features_pca[original_target2_label == label]\n",
    "    center = np.mean(cluster_data, axis=0)\n",
    "    cov_matrix = np.cov(cluster_data, rowvar=False)\n",
    "    cov_inv = np.linalg.pinv(cov_matrix) \n",
    "    cluster_stats[label] = {\n",
    "        \"center\": center,\n",
    "        \"cov_inv\": cov_inv\n",
    "    }\n",
    "\n",
    "# ===== calculate distance between target2_features_pca and test_target2_features_pca =====\n",
    "raw_drift_scores = []\n",
    "target1_drift_scores = []\n",
    "target2_drift_scores = []\n",
    "results = []\n",
    "full_results = []\n",
    "\n",
    "print(\"Semi-auto labelling to camera dust images\")\n",
    "for i, vec in enumerate(test_target2_features_pca):  \n",
    "    distances = {}\n",
    "    for label, stats in cluster_stats.items():\n",
    "        center = stats[\"center\"]\n",
    "        cov_inv = stats[\"cov_inv\"]\n",
    "        dist = distance.mahalanobis(vec, center, cov_inv)\n",
    "        distances[label] = dist\n",
    "\n",
    "    min_dist_label = min(distances, key=distances.get)\n",
    "    min_dist = distances[min_dist_label]\n",
    "\n",
    "    results.append({\n",
    "        \"index\": i,\n",
    "        \"nearest_cluster\": min_dist_label,\n",
    "        \"mahalanobis_distance\": round(min_dist, 4)\n",
    "    })\n",
    "    full_results.append({\n",
    "        \"index\": i,\n",
    "        \"distance_from_0\": distances[0],\n",
    "        \"distance_from_1\": distances[1],\n",
    "        \"distance_from_2\": distances[2],\n",
    "        \"distance_from_3\": distances[3],\n",
    "        \"distance_from_4\": distances[4],\n",
    "        \"distance_from_5\": distances[5],\n",
    "        \"distance_from_6\": distances[6]\n",
    "    })\n",
    "\n",
    "df_labels = pd.DataFrame(results)[[\"nearest_cluster\"]].rename(columns={\"nearest_cluster\": \"pred_label\"})\n",
    "\n",
    "save_path = r\"C:\\Users\\pass\\example.csv\"\n",
    "df_labels.to_csv(save_path, index=False, encoding=\"utf-8-sig\")  \n",
    "print(\"CSV saved:\", save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc63d36-86a5-4e9c-8007-9128f3bb187c",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58466c6b-4fdb-4718-99f4-8fbdaa453edf",
   "metadata": {},
   "source": [
    "## KRR+mahalanobis base labelling method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b610fd5-5c21-4412-8a14-3d00b45786e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Sinkhorn OT \n",
    "# =============================\n",
    "\n",
    "def _sqeuclidean_cost(X, Y):\n",
    "    \"\"\"\n",
    "    X: (Nx, d)\n",
    "    Y: (Ny, d)\n",
    "    return: C_ij = ||X_i - Y_j||^2  (Nx, Ny)\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, dtype=np.float64)\n",
    "    Y = np.asarray(Y, dtype=np.float64)\n",
    "    diff = X[:, None, :] - Y[None, :, :]\n",
    "    C = np.sum(diff**2, axis=2)\n",
    "    return C\n",
    "\n",
    "def sinkhorn_ot_barycentric(X, Y, eps=0.1, n_iter=200, tol=1e-9):\n",
    "    X = np.asarray(X, dtype=np.float64)\n",
    "    Y = np.asarray(Y, dtype=np.float64)\n",
    "    Nx, d = X.shape\n",
    "    Ny = Y.shape[0]\n",
    "\n",
    "    a = np.ones(Nx, dtype=np.float64) / Nx\n",
    "    b = np.ones(Ny, dtype=np.float64) / Ny\n",
    "\n",
    "    C = _sqeuclidean_cost(X, Y)  # (Nx, Ny)\n",
    "    K = np.exp(-C / eps)\n",
    "\n",
    "    K[K < 1e-300] = 1e-300\n",
    "\n",
    "    u = np.ones(Nx, dtype=np.float64)\n",
    "    v = np.ones(Ny, dtype=np.float64)\n",
    "\n",
    "    for _ in range(n_iter):\n",
    "        K_v = K @ v\n",
    "        K_v[K_v < 1e-300] = 1e-300     \n",
    "        u = a / K_v\n",
    "\n",
    "        K_t_u = K.T @ u\n",
    "        K_t_u[K_t_u < 1e-300] = 1e-300 \n",
    "        v = b / K_t_u\n",
    "\n",
    "    P = (u[:, None] * K) * v[None, :]\n",
    "\n",
    "    col_sums = P.sum(axis=0)\n",
    "    col_sums[col_sums < 1e-300] = 1e-300\n",
    "\n",
    "    X_tilde = (X.T @ P / col_sums[None, :]).T\n",
    "    return X_tilde, P\n",
    "\n",
    "\n",
    "# =============================\n",
    "# KRR\n",
    "# =============================\n",
    "\n",
    "def _rbf_kernel(Y1, Y2, sigma):\n",
    "    \"\"\"\n",
    "    RBF kernal K_ij = exp(-||y_i - y_j||^2 / (2 sigma^2))\n",
    "    Y1: (N1, d), Y2: (N2, d)\n",
    "    return: (N1, N2)\n",
    "    \"\"\"\n",
    "    Y1 = np.asarray(Y1, dtype=np.float64)\n",
    "    Y2 = np.asarray(Y2, dtype=np.float64)\n",
    "    diff = Y1[:, None, :] - Y2[None, :, :]\n",
    "    dist2 = np.sum(diff**2, axis=2)\n",
    "    K = np.exp(-dist2 / (2.0 * sigma**2))\n",
    "    return K\n",
    "\n",
    "def _safe_cov_simple(X, eps=1e-3):\n",
    "    \"\"\"\n",
    "    X: (N, d)\n",
    "    returan: covariance + eps * I\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, dtype=np.float64)\n",
    "    N = X.shape[0]\n",
    "    if N <= 1:\n",
    "        d = X.shape[1]\n",
    "        return eps * np.eye(d)\n",
    "    Sigma = np.cov(X, rowvar=False, bias=False)\n",
    "    d = Sigma.shape[0]\n",
    "    Sigma = Sigma + eps * np.eye(d)\n",
    "    return Sigma\n",
    "\n",
    "def krr_train_one_cluster(Y_train, X_train, sigma, lam_krr, eps_cov=1e-3, X_for_stats=None):\n",
    "    \"\"\"\n",
    "    Learn KRR for one cluster。\n",
    "    Y_train: (Ny, d) input（new data）\n",
    "    X_train: (Ny, d) output\n",
    "    sigma: width of RBF kernel\n",
    "    lam_krr: KRR normalization\n",
    "    eps_cov: ridge\n",
    "    return: model_k (dict)\n",
    "    \"\"\"\n",
    "    Y_train = np.asarray(Y_train, dtype=np.float64)\n",
    "    X_train = np.asarray(X_train, dtype=np.float64)\n",
    "    Ny, d = Y_train.shape\n",
    "\n",
    "    # Kernel\n",
    "    K = _rbf_kernel(Y_train, Y_train, sigma=sigma)  # (Ny, Ny)\n",
    "    A = K + lam_krr * np.eye(Ny)\n",
    "    Alpha = np.linalg.solve(A, X_train)             # (Ny, d)\n",
    "\n",
    "    if X_for_stats is None:\n",
    "        X_for_stats = X_train\n",
    "    X_for_stats = np.asarray(X_for_stats, dtype=np.float64)\n",
    "    mu_X = X_for_stats.mean(axis=0)\n",
    "    Sigma_X = _safe_cov_simple(X_for_stats, eps=eps_cov)\n",
    "\n",
    "    model_k = {\n",
    "        \"Y_train\": Y_train,\n",
    "        \"Alpha\": Alpha,\n",
    "        \"sigma\": sigma,\n",
    "        \"lam_krr\": lam_krr,\n",
    "        \"mu_X\": mu_X,\n",
    "        \"Sigma_X\": Sigma_X,\n",
    "    }\n",
    "    return model_k\n",
    "\n",
    "def krr_predict(model_k, Y_group):\n",
    "    \"\"\"\n",
    "    predict Y_group -> X_hat by using trained KRR\n",
    "    model_k: dict made by krr_train_one_cluster\n",
    "    Y_group: (M, d)\n",
    "    return: X_hat: (M, d)\n",
    "    \"\"\"\n",
    "    Y_group = np.asarray(Y_group, dtype=np.float64)\n",
    "    Y_train = model_k[\"Y_train\"]\n",
    "    Alpha = model_k[\"Alpha\"]\n",
    "    sigma = model_k[\"sigma\"]\n",
    "\n",
    "    K_new = _rbf_kernel(Y_group, Y_train, sigma=sigma)  # (M, Ny)\n",
    "    X_hat = K_new @ Alpha                               # (M, d)\n",
    "    return X_hat\n",
    "\n",
    "# =============================\n",
    "# learn Y->X for all 7 clusters by using OT+KRR\n",
    "# =============================\n",
    "\n",
    "def train_7_clusters_ot_krr(\n",
    "    X_clusters,           # raw data cluster: list of (Nx_k, d)\n",
    "    Y_clusters,           # new data cluster: list of (Ny_k, d)\n",
    "    sigma=10.0,\n",
    "    lam_krr=1e-3,\n",
    "    eps_cov=1e-3,\n",
    "    ot_eps=0.1,\n",
    "    ot_n_iter=200,\n",
    "):\n",
    "    \"\"\"\n",
    "    For each cluster K:\n",
    "      1. make Transportation Plan P_k between X_k and Y_k by using OT (Sinkhorn)\n",
    "      2. make Y_k -> X̃_k by using barycentric\n",
    "      3. learn f_k: Y -> X̃ by using KRR\n",
    "\n",
    "    return:\n",
    "      models_ot_krr: list [model_k]\n",
    "    \"\"\"\n",
    "    assert len(X_clusters) == 7 and len(Y_clusters) == 7\n",
    "    models = []\n",
    "\n",
    "    for k in range(7):\n",
    "        Xk = np.asarray(X_clusters[k], dtype=np.float64)\n",
    "        Yk = np.asarray(Y_clusters[k], dtype=np.float64)\n",
    "\n",
    "        X_tilde_k, P_k = sinkhorn_ot_barycentric(Xk, Yk, eps=ot_eps, n_iter=ot_n_iter)\n",
    "\n",
    "        mk = krr_train_one_cluster(\n",
    "            Y_train=Yk,\n",
    "            X_train=X_tilde_k,\n",
    "            sigma=sigma,\n",
    "            lam_krr=lam_krr,\n",
    "            eps_cov=eps_cov,\n",
    "            X_for_stats=Xk,\n",
    "        )\n",
    "        mk[\"k\"] = k\n",
    "        mk[\"ot_eps\"] = ot_eps\n",
    "        mk[\"ot_n_iter\"] = ot_n_iter\n",
    "        models.append(mk)\n",
    "\n",
    "    return models\n",
    "\n",
    "# =============================\n",
    "# Evaluatioon Function\n",
    "# =============================\n",
    "\n",
    "def evaluate_ot_krr_models_blockwise(\n",
    "    models_ot_krr,\n",
    "    X_clusters,    \n",
    "    Y_clusters,\n",
    "    use_common_sigma=False,\n",
    "    hybrid_alpha=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    evaluate models_ot_krr traied by OT+KRR\n",
    "    index:\n",
    "      - self_maha_mean\n",
    "      - self_euclid_mean\n",
    "      - self_is_min_rate\n",
    "    \"\"\"\n",
    "    K = len(models_ot_krr)\n",
    "    mus = [m[\"mu_X\"] for m in models_ot_krr]\n",
    "    sigmas = [m[\"Sigma_X\"] for m in models_ot_krr]\n",
    "\n",
    "    if use_common_sigma:\n",
    "        Sigma_common = sum(sigmas) / len(sigmas)\n",
    "    else:\n",
    "        Sigma_common = None\n",
    "\n",
    "    self_mahas = []\n",
    "    self_eucs  = []\n",
    "    is_min_flags = []\n",
    "\n",
    "    for k in range(K):\n",
    "        Yk = np.asarray(Y_clusters[k], dtype=np.float64)\n",
    "        X_hat = krr_predict(models_ot_krr[k], Yk)\n",
    "\n",
    "        mu_k = mus[k]\n",
    "        Sigma_k = sigmas[k]\n",
    "\n",
    "        if use_common_sigma:\n",
    "            maha_self = np.array([mahalanobis_sq(x, mu_k, Sigma_common) for x in X_hat])\n",
    "        else:\n",
    "            maha_self = np.array([mahalanobis_sq(x, mu_k, Sigma_k) for x in X_hat])\n",
    "\n",
    "        euc_self2 = np.sum((X_hat - mu_k[None, :])**2, axis=1)\n",
    "\n",
    "        all_scores = []\n",
    "        for j in range(K):\n",
    "            mu_j = mus[j]\n",
    "            if use_common_sigma:\n",
    "                maha_j = np.array([mahalanobis_sq(x, mu_j, Sigma_common) for x in X_hat])\n",
    "            else:\n",
    "                Sigma_j = sigmas[j]\n",
    "                maha_j = np.array([mahalanobis_sq(x, mu_j, Sigma_j) for x in X_hat])\n",
    "\n",
    "            if hybrid_alpha is not None:\n",
    "                euc_j2 = np.sum((X_hat - mu_j[None, :])**2, axis=1)\n",
    "                score_j = hybrid_alpha * maha_j + (1.0 - hybrid_alpha) * euc_j2\n",
    "            else:\n",
    "                score_j = maha_j\n",
    "            all_scores.append(score_j)\n",
    "\n",
    "        all_scores = np.stack(all_scores, axis=1)\n",
    "        pred_min = np.argmin(all_scores, axis=1)\n",
    "        is_min = (pred_min == k)\n",
    "\n",
    "        self_mahas.append(maha_self.mean())\n",
    "        self_eucs.append(np.sqrt(euc_self2).mean())\n",
    "        is_min_flags.append(is_min.mean())\n",
    "\n",
    "    res = {\n",
    "        \"self_maha_mean\": float(np.mean(self_mahas)),\n",
    "        \"self_euclid_mean\": float(np.mean(self_eucs)),\n",
    "        \"self_is_min_rate\": float(np.mean(is_min_flags)),\n",
    "    }\n",
    "    return res\n",
    "\n",
    "\n",
    "import json\n",
    "try:\n",
    "    import pandas as pd\n",
    "    _HAS_PANDAS = True\n",
    "except Exception:\n",
    "    _HAS_PANDAS = False\n",
    "\n",
    "def run_ot_krr_experiments(\n",
    "    X_clusters,\n",
    "    Y_clusters,\n",
    "    sigma_list=(10.0, 20.0),\n",
    "    lam_krr_list=(1e-3, 1e-2),\n",
    "    eps_cov_list=(1e-3,),\n",
    "    ot_eps_list=(0.1, 0.2),\n",
    "    ot_n_iter_list=(100,),\n",
    "    use_common_sigma=False,\n",
    "    hybrid_alpha=None,\n",
    "):\n",
    "    logs = []\n",
    "    best = None\n",
    "\n",
    "    total = (len(sigma_list) * len(lam_krr_list) *\n",
    "             len(eps_cov_list) * len(ot_eps_list) *\n",
    "             len(ot_n_iter_list))\n",
    "    print(f\"[OT+KRR] Total combinations: {total}\")\n",
    "    cnt = 0\n",
    "\n",
    "    for sigma in sigma_list:\n",
    "        for lam_krr in lam_krr_list:\n",
    "            for eps_cov in eps_cov_list:\n",
    "                for ot_eps in ot_eps_list:\n",
    "                    for ot_it in ot_n_iter_list:\n",
    "                        cnt += 1\n",
    "                        print(f\"  [{cnt}/{total}] sigma={sigma}, lam_krr={lam_krr}, \"\n",
    "                              f\"eps_cov={eps_cov}, ot_eps={ot_eps}, ot_n_iter={ot_it}\")\n",
    "\n",
    "                        models = train_7_clusters_ot_krr(\n",
    "                            X_clusters,\n",
    "                            Y_clusters,\n",
    "                            sigma=sigma,\n",
    "                            lam_krr=lam_krr,\n",
    "                            eps_cov=eps_cov,\n",
    "                            ot_eps=ot_eps,\n",
    "                            ot_n_iter=ot_it,\n",
    "                        )\n",
    "\n",
    "                        eval_res = evaluate_ot_krr_models_blockwise(\n",
    "                            models,\n",
    "                            X_clusters,\n",
    "                            Y_clusters,\n",
    "                            use_common_sigma=use_common_sigma,\n",
    "                            hybrid_alpha=hybrid_alpha,\n",
    "                        )\n",
    "\n",
    "\n",
    "                        v1 = eval_res.get(\"self_maha_mean\", np.nan)\n",
    "                        v2 = eval_res.get(\"self_euclid_mean\", np.nan)\n",
    "                        v3 = eval_res.get(\"self_is_min_rate\", np.nan)\n",
    "\n",
    "                        if (not np.isfinite(v1)) or (not np.isfinite(v2)) or (not np.isfinite(v3)):\n",
    "                            print(\"    → NaN / inf detected in eval_res, skip this combination.\")\n",
    "\n",
    "                            continue\n",
    "\n",
    "                        row = {\n",
    "                            \"sigma\": sigma,\n",
    "                            \"lam_krr\": lam_krr,\n",
    "                            \"eps_cov\": eps_cov,\n",
    "                            \"ot_eps\": ot_eps,\n",
    "                            \"ot_n_iter\": ot_it,\n",
    "                            **eval_res,\n",
    "                        }\n",
    "                        logs.append(row)\n",
    "\n",
    "                        if (best is None) or (eval_res[\"self_is_min_rate\"] > best[0][\"self_is_min_rate\"]):\n",
    "                            best = (eval_res, {\n",
    "                                \"sigma\": sigma,\n",
    "                                \"lam_krr\": lam_krr,\n",
    "                                \"eps_cov\": eps_cov,\n",
    "                                \"ot_eps\": ot_eps,\n",
    "                                \"ot_n_iter\": ot_it,\n",
    "                            })\n",
    "\n",
    "    if best is None:\n",
    "        print(\"there is no valid result\")\n",
    "        best_out = None\n",
    "    else:\n",
    "        best_eval, best_params = best\n",
    "        best_out = {**best_params, **best_eval}\n",
    "\n",
    "    return logs, best_out\n",
    "\n",
    "def ot_krr_logs_to_df(logs):\n",
    "    if _HAS_PANDAS:\n",
    "        return pd.DataFrame(logs)\n",
    "    return logs\n",
    "\n",
    "# =============================\n",
    "# semi-auto labelling to Y_new\n",
    "# =============================\n",
    "\n",
    "def semi_auto_label_with_ot_krr(\n",
    "    Y_new,            # (M, d) \n",
    "    models_ot_krr,    # 7models trained by train_7_clusters_ot_krr\n",
    "    use_common_sigma=False,\n",
    "    hybrid_alpha=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    For Y_new:\n",
    "      calculate X_hat_k = f_k(Y_new) by using KRR model f_k\n",
    "      X_hat_k is labelled based on nearest center of cluster\n",
    "\n",
    "    return:\n",
    "      pred_labels: (M,)    prediction mode（0〜6）\n",
    "      min_scores:  (M,)    minimum score for the prediction mode\n",
    "      all_scores:  (M, 7)  all scores\n",
    "    \"\"\"\n",
    "    Y_new = np.asarray(Y_new, dtype=np.float64)\n",
    "    M, d = Y_new.shape\n",
    "    K = len(models_ot_krr)\n",
    "\n",
    "    mus = [m[\"mu_X\"] for m in models_ot_krr]\n",
    "    sigmas = [m[\"Sigma_X\"] for m in models_ot_krr]\n",
    "\n",
    "    if use_common_sigma:\n",
    "        Sigma_common = sum(sigmas) / len(sigmas)\n",
    "    else:\n",
    "        Sigma_common = None\n",
    "\n",
    "    all_scores = []\n",
    "\n",
    "    for k in range(K):\n",
    "        X_hat_k = krr_predict(models_ot_krr[k], Y_new)  # (M, d)\n",
    "        mu_k = mus[k]\n",
    "        Sigma_k = sigmas[k]\n",
    "\n",
    "        if use_common_sigma:\n",
    "            maha_k = np.array([mahalanobis_sq(x, mu_k, Sigma_common) for x in X_hat_k])\n",
    "        else:\n",
    "            maha_k = np.array([mahalanobis_sq(x, mu_k, Sigma_k) for x in X_hat_k])\n",
    "\n",
    "        if hybrid_alpha is not None:\n",
    "            euc2_k = np.sum((X_hat_k - mu_k[None, :])**2, axis=1)\n",
    "            score_k = hybrid_alpha * maha_k + (1.0 - hybrid_alpha) * euc2_k\n",
    "        else:\n",
    "            score_k = maha_k\n",
    "\n",
    "        all_scores.append(score_k)\n",
    "\n",
    "    all_scores = np.stack(all_scores, axis=1)  # (M, K)\n",
    "\n",
    "    pred_labels = np.argmin(all_scores, axis=1)     # (M,)\n",
    "    min_scores = np.min(all_scores, axis=1)         # (M,)\n",
    "\n",
    "    return pred_labels, min_scores, all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725af2fe-546a-4e5c-b7f3-ccc2f2d17b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#semi-auto labelling to bright images\n",
    "\n",
    "# 1. prepare clusters\n",
    "#    X side: features_pca\n",
    "#    Y side: target1_features_pca\n",
    "X_clusters1, Y_clusters1 = prepare_clusters(\n",
    "    features_pca,\n",
    "    target1_features_pca,\n",
    "    n_src=240,  # the number of X/mode\n",
    "    n_tgt=48,   # the number of Y/mode\n",
    "    K=7\n",
    ")\n",
    "\n",
    "# 2. parameter sweep about OT+KRR\n",
    "ot_krr_logs1, ot_krr_best1 = run_ot_krr_experiments(\n",
    "    X_clusters1, Y_clusters1,\n",
    "    sigma_list=(10.0, 20.0),\n",
    "    lam_krr_list=(1e-3, 1e-2),\n",
    "    eps_cov_list=(1e-3,),\n",
    "    ot_eps_list=(1.0, 2.0, 5.0),\n",
    "    ot_n_iter_list=(100,),\n",
    "    use_common_sigma=False,\n",
    "    hybrid_alpha=None,\n",
    ")\n",
    "print(\"OT+KRR Best:\", ot_krr_best1)\n",
    "df_otkrr1 = ot_krr_logs_to_df(ot_krr_logs1)\n",
    "\n",
    "# 3. train final model by using the most proper parameter\n",
    "bp1 = ot_krr_best1\n",
    "models_ot_krr_best1 = train_7_clusters_ot_krr(\n",
    "    X_clusters1,\n",
    "    Y_clusters1,\n",
    "    sigma=bp1[\"sigma\"],\n",
    "    lam_krr=bp1[\"lam_krr\"],\n",
    "    eps_cov=bp1[\"eps_cov\"],\n",
    "    ot_eps=bp1[\"ot_eps\"],\n",
    "    ot_n_iter=bp1[\"ot_n_iter\"],\n",
    ")\n",
    "\n",
    "# 4. semi-auto labelling to Y_new_features_pca\n",
    "#    Y_new_features_pca: (M, 100) without label\n",
    "Y_new_features_pca1 = test_target1_features_pca\n",
    "pred_labels, min_scores, all_scores = semi_auto_label_with_ot_krr(\n",
    "    Y_new_features_pca1,\n",
    "    models_ot_krr_best1,\n",
    "    use_common_sigma=False,\n",
    "    hybrid_alpha=None,  \n",
    ")\n",
    "\n",
    "min_maha_dist = np.sqrt(min_scores)  # (M,)\n",
    "\n",
    "df_result = pd.DataFrame({\n",
    "    \"pred_label\": pred_labels,\n",
    "    \"min_maha_sq\": min_scores,\n",
    "    \"min_maha\": np.sqrt(min_scores),\n",
    "})\n",
    "# CSV output\n",
    "save_path = r\"C:\\Users\\pass\\example.csv\"\n",
    "df_result.to_csv(save_path, index=False, encoding=\"utf-8\")\n",
    "print(\"CSV saved:\", save_path)\n",
    "\n",
    "df_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e18c03-9e9a-4cf5-ba77-4a03dc6f61d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#semi-auto labelling to camera dust images\n",
    "\n",
    "# 1. prepare clusters\n",
    "#    X side: features_pca\n",
    "#    Y side: target1_features_pca\n",
    "X_clusters2, Y_clusters2 = prepare_clusters(\n",
    "    features_pca,\n",
    "    target2_features_pca,\n",
    "    n_src=240,  # the number of X/mode\n",
    "    n_tgt=48,   # the number of Y/mode\n",
    "    K=7\n",
    ")\n",
    "\n",
    "# 2. parameter sweep about OT+KRR\n",
    "ot_krr_logs2, ot_krr_best2 = run_ot_krr_experiments(\n",
    "    X_clusters2, Y_clusters2,\n",
    "    sigma_list=(10.0, 20.0),\n",
    "    lam_krr_list=(1e-3, 1e-2),\n",
    "    eps_cov_list=(1e-3,),\n",
    "    ot_eps_list=(1.0, 2.0, 5.0),\n",
    "    ot_n_iter_list=(100,),\n",
    "    use_common_sigma=False,\n",
    "    hybrid_alpha=None,\n",
    ")\n",
    "print(\"OT+KRR Best:\", ot_krr_best2)\n",
    "df_otkrr2 = ot_krr_logs_to_df(ot_krr_logs2)\n",
    "\n",
    "# 3. train final model by using the most proper parameter\n",
    "bp2 = ot_krr_best2\n",
    "models_ot_krr_best2 = train_7_clusters_ot_krr(\n",
    "    X_clusters2,\n",
    "    Y_clusters2,\n",
    "    sigma=bp2[\"sigma\"],\n",
    "    lam_krr=bp2[\"lam_krr\"],\n",
    "    eps_cov=bp2[\"eps_cov\"],\n",
    "    ot_eps=bp2[\"ot_eps\"],\n",
    "    ot_n_iter=bp2[\"ot_n_iter\"],\n",
    ")\n",
    "\n",
    "# 4. semi-auto labelling to Y_new_features_pca\n",
    "#    Y_new_features_pca: (M, 100) without label\n",
    "Y_new_features_pca2 = test_target2_features_pca\n",
    "pred_labels, min_scores, all_scores = semi_auto_label_with_ot_krr(\n",
    "    Y_new_features_pca2,\n",
    "    models_ot_krr_best2,\n",
    "    use_common_sigma=False,\n",
    "    hybrid_alpha=None,  \n",
    ")\n",
    "\n",
    "min_maha_dist = np.sqrt(min_scores)  # (M,)\n",
    "\n",
    "df_result = pd.DataFrame({\n",
    "    \"pred_label\": pred_labels,\n",
    "    \"min_maha_sq\": min_scores,\n",
    "    \"min_maha\": np.sqrt(min_scores),\n",
    "})\n",
    "# CSV output\n",
    "save_path = r\"C:\\Users\\pass\\example.csv\"\n",
    "df_result.to_csv(save_path, index=False, encoding=\"utf-8\")\n",
    "print(\"CSV saved:\", save_path)\n",
    "\n",
    "df_result.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
