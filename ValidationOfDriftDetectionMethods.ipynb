{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13b76b31-739f-4c43-a8bc-36694094f2cd",
   "metadata": {},
   "source": [
    "# Validation of drift detection methods\n",
    "## Comparing Proposed method(mahalanobis distance base) and Conventional method(prediction possibility base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246ff24f-2651-4a2e-935c-aa6bd5452312",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113eeb9e-d4a8-49a5-bd3a-15547ab81791",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c71b9d-f837-4d69-a93b-19a28402da61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten, Input, Lambda\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import glob\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import time\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from keras.models import Model\n",
    "from gradcamutils import GradCam, GradCamPlusPlus, ScoreCam, GuidedBackPropagation, superimpose, read_and_preprocess_img, build_guided_model\n",
    "\n",
    "from affine_a_method_det import set_deterministic, fit_affine_A_method_deterministic, inverse_transform_ridge, mahalanobis_sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55c22e9-7fbe-4ad3-8dc6-36aaf258d28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fuction for making label\n",
    "def create_image_labels(n_light, n_water, n_blackline, n_discoloration, n_dotsonline, n_adhesion, n_scratch):\n",
    "    \"\"\"\n",
    "    function for making label corresponding to each abnormal mode\n",
    "    \n",
    "    Args:\n",
    "        n_light (int): the number of light images\n",
    "        n_water (int): the number of water images\n",
    "        n_blackline (int): the number of black line images\n",
    "        n_discoloration (int): the number of discoloration images\n",
    "        n_dotsonline (int): the number of dots on line images\n",
    "        n_adhesion (int): the number of adhesion images\n",
    "        n_scrtch (int): the number of surface scratches images\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: 1collumn=image„ÄÅ2collumn=label\n",
    "    \"\"\"\n",
    "    # Name list for each abnormal mode\n",
    "    light_data = [f\"Light-{i}\" for i in range(1, n_light + 1)]\n",
    "    water_data = [f\"Water-{i}\" for i in range(1, n_water + 1)]\n",
    "    blackline_data = [f\"BlackLine-{i}\" for i in range(1, n_blackline + 1)]\n",
    "    discoloration_data = [f\"Discoloration-{i}\" for i in range(1, n_discoloration + 1)]\n",
    "    dotsonline_data = [f\"DotsOnLine-{i}\" for i in range(1, n_dotsonline + 1)]\n",
    "    copper_data = [f\"Adhesion-{i}\" for i in range(1, n_adhesion + 1)]\n",
    "    spark_data = [f\"SurfaceScratch-{i}\" for i in range(1, n_scratch + 1)]\n",
    "\n",
    "    # Label list\n",
    "    light_labels = [0] * n_light\n",
    "    water_labels = [1] * n_water\n",
    "    blackline_labels = [2] * n_blackline\n",
    "    discoloration_labels = [3] * n_discoloration\n",
    "    dotsonline_labels = [4] * n_dotsonline\n",
    "    adhesion_labels = [5] * n_adhesion\n",
    "    scratch_labels = [6] * n_scratch\n",
    "\n",
    "    # make dataframe by combining data\n",
    "    data = list(zip(light_data + water_data + blackline_data + discoloration_data + dotsonline_data + copper_data + spark_data, \n",
    "                    light_labels + water_labels + blackline_labels + discoloration_labels + dotsonline_labels + copper_labels + spark_labels))\n",
    "    df = pd.DataFrame(data, columns=[\"image\", \"label\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_7mode_images(src_dirs):\n",
    "    \"\"\"\n",
    "    src_dirs: list containing full pass of seven folders\n",
    "              ex: [\n",
    "                   r\"C:/path/mode0/*jpg\",\n",
    "                   r\"C:/path/mode1/*jpg\",\n",
    "                   ...\n",
    "                  ]\n",
    "\n",
    "    return:\n",
    "      all_images      :list of all images(cv2) \n",
    "      images_by_class : list of each abnormal mode images [list0, list1, ..., list6]\n",
    "      labels          : list of labels corresponding to each image\n",
    "      nums            : list of the number of each class images [n0,n1,...,n6]\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(src_dirs) == 7, \"Designate 7 folder pass\"\n",
    "\n",
    "    images_by_class = []\n",
    "    nums = []\n",
    "\n",
    "    # process\n",
    "    for i, path in enumerate(src_dirs):\n",
    "        filepaths = glob.glob(path)\n",
    "        print(f\"Class {i}: {len(filepaths)} files\")\n",
    "\n",
    "        imgs = []\n",
    "        for fp in filepaths:\n",
    "            img = cv2.imread(fp)\n",
    "            if img is not None:\n",
    "                imgs.append(img)\n",
    "\n",
    "        images_by_class.append(imgs)\n",
    "        nums.append(len(imgs))\n",
    "\n",
    "    # combine\n",
    "    all_images = []\n",
    "    for cls_imgs in images_by_class:\n",
    "        all_images.extend(cls_imgs)\n",
    "\n",
    "    # make label\n",
    "    labels = create_image_labels(nums) \n",
    "\n",
    "    return all_images, images_by_class, labels, nums\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407f8382-ed5b-41a6-a568-c80aacb976c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get pass\n",
    "\n",
    "#wire raw images\n",
    "#TrainingData\n",
    "source_dirs = [\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "]\n",
    "#TestData\n",
    "source_test_dirs = [\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "]\n",
    "\n",
    "#bright images\n",
    "#TrainingData\n",
    "target1_dirs = [\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "]\n",
    "#TestData\n",
    "test_target1_dirs = [\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "]\n",
    "\n",
    "#camera dust images\n",
    "#TrainingData\n",
    "target2_dirs = [\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "]\n",
    "#TestData\n",
    "test_target2_dirs = [\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "    r\"C:\\Users\\pass\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a127a92-e7cf-43ae-a2a1-02026c21257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data and label\n",
    "source_all_imgs, source_imgs_by_class, source_label, source_num = load_7mode_images(source_dirs)\n",
    "source_test_all_imgs, source_test_imgs_by_class, source_test_label, source_test_num = load_7mode_images(source_test_dirs)\n",
    "target1_all_imgs, target1_imgs_by_class, target1_label, target1_num = load_7mode_images(target1_dirs)\n",
    "test_target1_all_imgs, test_target1_imgs_by_class, test_target1_label, test_target1_num = load_7mode_images(test_target1_dirs)\n",
    "target2_all_imgs, target2_imgs_by_class, target2_label, target2_num = load_7mode_images(target2_dirs)\n",
    "test_target2_all_imgs, test_target2_imgs_by_class, test_target2_label, test_target2_num = load_7mode_images(test_target2_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da397bc5-b1c0-40bd-869f-e73989d12a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalization of images\n",
    "sourcefile_list = [file.astype(float)/255 for file in source_all_imgs]\n",
    "sourcefile_list = [cv2.resize(file, (360, 270)) for file in sourcefile_list]\n",
    "sourcefile_test_list = [file.astype(float)/255 for file in source_test_all_imgs]\n",
    "sourcefile_test_list = [cv2.resize(file, (360, 270)) for file in sourcefile_test_list]\n",
    "targetfile1_list = [file.astype(float)/255 for file in target1_all_imgs]\n",
    "targetfile1_list = [cv2.resize(file, (360, 270)) for file in targetfile1_list]\n",
    "test_targetfile1_list = [file.astype(float)/255 for file in test_target1_all_imgs]\n",
    "test_targetfile1_list = [cv2.resize(file, (360, 270)) for file in test_targetfile1_list]\n",
    "targetfile2_list = [file.astype(float)/255 for file in target2_all_imgs]\n",
    "targetfile2_list = [cv2.resize(file, (360, 270)) for file in targetfile2_list]\n",
    "test_targetfile2_list = [file.astype(float)/255 for file in test_target2_all_imgs]\n",
    "test_targetfile2_list = [cv2.resize(file, (360, 270)) for file in test_targetfile2_list]\n",
    "\n",
    "#numpy list\n",
    "original_source_label = source_label[\"label\"]\n",
    "original_source_label = np.array(original_source_label)\n",
    "original_source_test_label = source_test_label[\"label\"]\n",
    "original_source_test_label = np.array(original_source_test_label)\n",
    "original_target1_label = target1_label[\"label\"]\n",
    "original_target1_label = np.array(original_target1_label)\n",
    "original_test_target1_label = test_target1_label[\"label\"]\n",
    "original_test_target1_label = np.array(original_test_target1_label)\n",
    "original_target2_label = target2_label[\"label\"]\n",
    "original_target2_label = np.array(original_target2_label)\n",
    "original_test_target2_label = test_target2_label[\"label\"]\n",
    "original_test_target2_label = np.array(original_test_target2_label)\n",
    "\n",
    "\n",
    "#dummy parameter \n",
    "source_label = to_categorical(source_label[\"label\"])\n",
    "source_test_label = to_categorical(source_test_label[\"label\"])\n",
    "target1_label = to_categorical(target1_label[\"label\"])\n",
    "test_target1_label = to_categorical(test_target1_label[\"label\"])\n",
    "target2_label = to_categorical(target2_label[\"label\"])\n",
    "test_target2_label = to_categorical(test_target2_label[\"label\"])\n",
    "\n",
    "#change the data to numpy list\n",
    "#save original data\n",
    "raw_sourcefile_list = sourcefile_list\n",
    "raw_sourcefile_test_list = sourcefile_test_list\n",
    "raw_targetfile1_list = targetfile1_list\n",
    "raw_test_targetfile1_list = test_targetfile1_list\n",
    "raw_targetfile2_list = targetfile2_list\n",
    "raw_test_targetfile2_list = test_targetfile2_list\n",
    "\n",
    "#numpy list\n",
    "sourcefile_list = np.array(sourcefile_list)\n",
    "sourcefile_test_list = np.array(sourcefile_test_list)\n",
    "targetfile1_list = np.array(targetfile1_list)\n",
    "test_targetfile1_list = np.array(test_targetfile1_list)\n",
    "targetfile2_list = np.array(targetfile2_list)\n",
    "test_targetfile2_list = np.array(test_targetfile2_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb818ddb-5368-4f08-8b22-477d549e25b8",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d644384-712a-4be0-94c8-606c51d65d52",
   "metadata": {},
   "source": [
    "## Proposed method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdbc473-b2e4-4636-983e-4318687db90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read base model\n",
    "base_model = load_model('BaseModel.h5')\n",
    "# separate feature extractor and output layer\n",
    "feature_output = base_model.layers[-5].output\n",
    "\n",
    "# GlobalAveragePooling\n",
    "pooled_output = layers.GlobalAveragePooling2D()(feature_output)\n",
    "\n",
    "# make feature extractor from base model\n",
    "feature_extractor = Model(inputs=base_model.input, outputs=pooled_output)\n",
    "feature_extractor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50c8bcf-6e2c-4120-b6bf-cb88ecc0d6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract features(feature vector) from each image\n",
    "def extract_features(model, images):\n",
    "    return model.predict(images, batch_size=32)\n",
    "\n",
    "source_features = extract_features(feature_extractor, sourcefile_list)\n",
    "target1_features = extract_features(feature_extractor, targetfile1_list)\n",
    "target2_features = extract_features(feature_extractor, targetfile2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884e2df3-1255-4fb1-8fb3-264d98f496b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce the dimension from 1024 to 100\n",
    "n_comp = 100\n",
    "pca = PCA(n_components=n_comp, svd_solver='full', random_state=42)\n",
    "features_pca = pca.fit_transform(source_features)\n",
    "# mapping data to the pca space made above\n",
    "target1_features_pca = pca.transform(target1_features)\n",
    "target2_features_pca = pca.transform(target2_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4cc870-53ab-4328-bf08-de5d44a926e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== calculate center of each cluster =====\n",
    "cluster_stats = {}\n",
    "for label in np.unique(original_source_label):\n",
    "    cluster_data = features_pca[original_source_label == label]\n",
    "    center = np.mean(cluster_data, axis=0)\n",
    "    cov_matrix = np.cov(cluster_data, rowvar=False)\n",
    "    cov_inv = np.linalg.pinv(cov_matrix)  \n",
    "    cluster_stats[label] = {\n",
    "        \"center\": center,\n",
    "        \"cov_inv\": cov_inv\n",
    "    }\n",
    "\n",
    "# ===== calculate mahalanobis distance as drift =====\n",
    "raw_drift_scores = []\n",
    "target_drift_scores = []\n",
    "provided_drift_scores = []\n",
    "results = []\n",
    "full_results = []\n",
    "\n",
    "# list containing minimum mahalanobis distance for each data\n",
    "orig_min_dists = []   # raw images\n",
    "gamma_min_dists = []  # bright images\n",
    "dirt_min_dists = []   # camera dust images\n",
    "\n",
    "\n",
    "#calculate drift for raw images\n",
    "for i, vec in enumerate(features_pca):\n",
    "    distances = {}\n",
    "    for label, stats in cluster_stats.items():\n",
    "        center = stats[\"center\"]\n",
    "        cov_inv = stats[\"cov_inv\"]\n",
    "        dist = distance.mahalanobis(vec, center, cov_inv)\n",
    "        distances[label] = dist\n",
    "\n",
    "    min_dist_label = min(distances, key=distances.get)\n",
    "    min_dist = distances[min_dist_label]\n",
    "\n",
    "    orig_min_dists.append(min_dist)\n",
    "\n",
    "    results.append({\n",
    "        \"index\": i,\n",
    "        \"nearest_cluster\": min_dist_label,\n",
    "        \"mahalanobis_distance\": round(min_dist, 4)\n",
    "    })\n",
    "    full_results.append({\n",
    "        \"index\": i,\n",
    "        \"distance_from_0\": distances[0],\n",
    "        \"distance_from_1\": distances[1],\n",
    "        \"distance_from_2\": distances[2],\n",
    "        \"distance_from_3\": distances[3],\n",
    "        \"distance_from_4\": distances[4],\n",
    "        \"distance_from_5\": distances[5],\n",
    "        \"distance_from_6\": distances[6]\n",
    "    })\n",
    "\n",
    "#calculate drift for bright images\n",
    "for i, vec in enumerate(all_target1_300_features_pca):\n",
    "    distances = {}\n",
    "    for label, stats in cluster_stats.items():\n",
    "        center = stats[\"center\"]\n",
    "        cov_inv = stats[\"cov_inv\"]\n",
    "        dist = distance.mahalanobis(vec, center, cov_inv)\n",
    "        distances[label] = dist\n",
    "\n",
    "    min_dist_label = min(distances, key=distances.get)\n",
    "    min_dist = distances[min_dist_label]\n",
    "\n",
    "    gamma_min_dists.append(min_dist)\n",
    "\n",
    "    results.append({\n",
    "        \"index\": i,\n",
    "        \"nearest_cluster\": min_dist_label,\n",
    "        \"mahalanobis_distance\": round(min_dist, 4)\n",
    "    })\n",
    "    full_results.append({\n",
    "        \"index\": i,\n",
    "        \"distance_from_0\": distances[0],\n",
    "        \"distance_from_1\": distances[1],\n",
    "        \"distance_from_2\": distances[2],\n",
    "        \"distance_from_3\": distances[3],\n",
    "        \"distance_from_4\": distances[4],\n",
    "        \"distance_from_5\": distances[5],\n",
    "        \"distance_from_6\": distances[6]\n",
    "    })\n",
    "\n",
    "#calculate drift for camera dust images\n",
    "for i, vec in enumerate(all_target2_300_features_pca):\n",
    "    distances = {}\n",
    "    for label, stats in cluster_stats.items():\n",
    "        center = stats[\"center\"]\n",
    "        cov_inv = stats[\"cov_inv\"]\n",
    "        dist = distance.mahalanobis(vec, center, cov_inv)\n",
    "        distances[label] = dist\n",
    "\n",
    "    min_dist_label = min(distances, key=distances.get)\n",
    "    min_dist = distances[min_dist_label]\n",
    "\n",
    "    dirt_min_dists.append(min_dist)\n",
    "\n",
    "    results.append({\n",
    "        \"index\": i,\n",
    "        \"nearest_cluster\": min_dist_label,\n",
    "        \"mahalanobis_distance\": round(min_dist, 4)\n",
    "    })\n",
    "    full_results.append({\n",
    "        \"index\": i,\n",
    "        \"distance_from_0\": distances[0],\n",
    "        \"distance_from_1\": distances[1],\n",
    "        \"distance_from_2\": distances[2],\n",
    "        \"distance_from_3\": distances[3],\n",
    "        \"distance_from_4\": distances[4],\n",
    "        \"distance_from_5\": distances[5],\n",
    "        \"distance_from_6\": distances[6]\n",
    "    })\n",
    "\n",
    "\n",
    "# save as CSV\n",
    "pd.DataFrame(results).to_csv(\"mahalanobis_cluster_distances.csv\", index=False)\n",
    "pd.DataFrame(full_results).to_csv(\"mahalanobis_full_cluster_distances.csv\", index=False)\n",
    "print(\"output done: mahalanobis_cluster_distances.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dd8157-3a24-4869-85f1-5f958443c408",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6f8f44-5c65-4c13-ba86-4c4bdfffc6ae",
   "metadata": {},
   "source": [
    "## Conventional method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7183a8-e7b1-4bc4-a3c5-63a8970abc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate prediction possibility\n",
    "\n",
    "#get prediction result from base model\n",
    "source_predictions = base_model.predict(sourcefile_list)\n",
    "target1_predictions = base_model.predict(targetfile1_list)\n",
    "target2_predictions = base_model.predict(targetfile2_list)\n",
    "\n",
    "#get predicition possobility\n",
    "source_possibility_results = source_predictions.max(axis = 1)\n",
    "target1_possibility_results = target1_predictions.max(axis = 1)\n",
    "target2_possibility_results = target2_predictions.max(axis = 1)\n",
    "\n",
    "#save\n",
    "np.savetxt(\"source_possibility_results.csv\", source_possibility_results)\n",
    "np.savetxt(\"target1_possibility_results.csv\", target1_possibility_results)\n",
    "np.savetxt(\"target2_possibility_results.csv\", target2_possibility_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
